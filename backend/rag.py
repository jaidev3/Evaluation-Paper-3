from dotenv import load_dotenv
load_dotenv()

# from langch


# load the document
# extract the text

# split the text/ chunking
# embedding 
# store to chromedb
# on hitting endpoint of chat it will cal rag
# embedd the query asked by user 
# retrive the most similar chunks
#  augment to llm along with query
# get answer 
# send in response
